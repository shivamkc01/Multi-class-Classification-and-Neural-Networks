{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-class Classification and Neural Networks",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIfwK-ygjHrP"
      },
      "source": [
        "# Implement one-vs-all logistic regression and neural networks to recognize hand-written digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiUkVkAyTFUv"
      },
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGo2OFAZVrhu"
      },
      "source": [
        "data  = loadmat('/content/ex4data1.mat')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feaOvQYSVxGT",
        "outputId": "784fd93c-49a2-45e7-d7d9-6630bbafde8e"
      },
      "source": [
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " '__globals__': [],\n",
              " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
              " '__version__': '1.0',\n",
              " 'y': array([[10],\n",
              "        [10],\n",
              "        [10],\n",
              "        ...,\n",
              "        [ 9],\n",
              "        [ 9],\n",
              "        [ 9]], dtype=uint8)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juH9Q1cPVzgL",
        "outputId": "2c01ce3b-c4ef-4719-abd9-da896a2c72c4"
      },
      "source": [
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "X.shape, y.shape ## look at the dimensions"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 400), (5000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkUfAtT9Wk5i"
      },
      "source": [
        "We also need to perform one-hot encoding on our y tag. The one-hot encoding converts the class label n (k class) into a vector of length k, where the index n is \"hot\" (1), and the rest are 0. Scikitlearn has a built-in utility, we can use this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-lam-v1WFPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1585a7-9d3d-445c-a344-1d28b23f6b12"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "y_onehot.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbwe2LwzW2MW",
        "outputId": "18f04aca-3899-42c1-90bd-e26ad862064a"
      },
      "source": [
        "y[0], y_onehot[0,:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([10], dtype=uint8), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lv3Fbb3XWtL"
      },
      "source": [
        "The neural network we are going to build for this exercise has an input layer that matches the size of our instance data (400 + bias units), a hidden layer of 25 units (26 with bias units), and an output layer, 10 units correspond to one of our one-hot encoding labels. \n",
        "\n",
        "The first thing we need to achieve is to evaluate the cost function of the loss of a given set of network parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFqvt2m5W89K"
      },
      "source": [
        "## Here we use Sigmoid Function\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6AJjI-pXqzs"
      },
      "source": [
        "\n",
        "## creating a forward Propagate function\n",
        "def forward_propagate(X, val1, val2):\n",
        "    m = X.shape[0]\n",
        "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
        "    z2 = a1 * val1.T\n",
        "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
        "    z3 = a2 * val2.T\n",
        "    h = sigmoid(z3)\n",
        "    \n",
        "    return a1, z2, a2, z3, h\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7NWAhsyYZXz"
      },
      "source": [
        "def cost(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    val1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    val2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "    \n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, val1, val2)\n",
        "    \n",
        "    # compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    return J\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXhAalaaK0g"
      },
      "source": [
        "We have used this Sigmoid function before. The forward propagation function calculates the hypothesis for each training instance given the current parameters. Its output shape should be the same as a one-hot encoding of y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLKQtcD9aCUF",
        "outputId": "99b5f059-66bb-4095-8083-30d86e6c33b9"
      },
      "source": [
        "# Initialize settings\n",
        "input_size = 400\n",
        "hidden_size = 25\n",
        "num_labels = 10\n",
        "learning_rate = 1\n",
        "\n",
        "# Randomly initialize the parameter array of the complete network parameter size.\n",
        "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
        "\n",
        "m = X.shape[0]\n",
        "X = np.matrix(X)\n",
        "y = np.matrix(y)\n",
        "# Unpack the parameter array into the parameter matrix of each layer\n",
        "val1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "val2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "val1.shape, val2.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25, 401), (10, 26))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw55wTuUbbOF",
        "outputId": "acc829be-f052-414b-e9db-6e90ce9a4320"
      },
      "source": [
        "a1, z2, a2, z3, h = forward_propagate(X, val1, val2)\n",
        "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 401), (5000, 25), (5000, 26), (5000, 10), (5000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6er-TFbc7CW"
      },
      "source": [
        "Cost function After calculating the hypothesis matrix h, the cost function is applied to calculate the total error between y and h.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGrhtY_mbgUH",
        "outputId": "76dc2eac-786f-439e-8f22-c53b1ea1a3eb"
      },
      "source": [
        "cost(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.687350525309224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksZeiYHQeHXg"
      },
      "source": [
        "Next is the back propagation algorithm. The backpropagation parameter update calculation will reduce the network error on the training data. The first thing we need is a function to calculate the gradient of the Sigmoid function we created earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3TstrrEc9X3"
      },
      "source": [
        "def sigmoid_gradient(z):\n",
        "  return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1C0x_w7eV2Q"
      },
      "source": [
        "Now we are ready to implement backpropagation to calculate the gradient. Since the calculation required for backpropagation is the calculation process required in the cost function, we will actually extend the cost function to perform backpropagation and return the cost and gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8p6_S-9eRgZ"
      },
      "source": [
        "def backprop(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    val1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    val2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "    \n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, val1, val2)\n",
        "    \n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(val1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(val2.shape)  # (10, 26)\n",
        "    \n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(val1[:,1:], 2)) + np.sum(np.power(val2[:,1:], 2)))\n",
        "    \n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]  # (1, 401)\n",
        "        z2t = z2[t,:]  # (1, 25)\n",
        "        a2t = a2[t,:]  # (1, 26)\n",
        "        ht = h[t,:]  # (1, 10)\n",
        "        yt = y[t,:]  # (1, 10)\n",
        "        \n",
        "        d3t = ht - yt  # (1, 10)\n",
        "        \n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)\n",
        "        d2t = np.multiply((val2.T * d3t.T).T, sigmoid_gradient(z2t))  # (1, 26)\n",
        "        \n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + d3t.T * a2t\n",
        "        \n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    \n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
        "    \n",
        "    return J, grad"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXbM8rkKewR0"
      },
      "source": [
        "The hardest part of backpropagation calculations (other than understanding why we are doing all these calculations) is to get the correct matrix dimensions. By the way, you can easily confuse the use of A * B with np.multiply(A, B). Basically the former is matrix multiplication and the latter is element-wise multiplication (unless A or B is a scalar value, in which case it doesn't matter). Anyway, let's test it to make sure the function returns what we expect.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrouXblyesEr",
        "outputId": "b6b33451-2389-4424-8dc9-2126c4d50582"
      },
      "source": [
        "J, grad = backprop(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.692664026112636, (10285,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9FqUcLlfOR4"
      },
      "source": [
        "We also need to make a modification to the back propagation function, that is, adding regularization to the gradient calculation. The final official version is as follows.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuOpRBXje1wZ"
      },
      "source": [
        "def backprop(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    val1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    val2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "    \n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, val1, val2)\n",
        "    \n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(val1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(val2.shape)  # (10, 26)\n",
        "    \n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(val1[:,1:], 2)) + np.sum(np.power(val2[:,1:], 2)))\n",
        "    \n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]  # (1, 401)\n",
        "        z2t = z2[t,:]  # (1, 25)\n",
        "        a2t = a2[t,:]  # (1, 26)\n",
        "        ht = h[t,:]  # (1, 10)\n",
        "        yt = y[t,:]  # (1, 10)\n",
        "        \n",
        "        d3t = ht - yt  # (1, 10)\n",
        "        \n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)\n",
        "        d2t = np.multiply((val2.T * d3t.T).T, sigmoid_gradient(z2t))  # (1, 26)\n",
        "        \n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + d3t.T * a2t\n",
        "        \n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    \n",
        "    # add the gradient regularization term\n",
        "    delta1[:,1:] = delta1[:,1:] + (val1[:,1:] * learning_rate) / m\n",
        "    delta2[:,1:] = delta2[:,1:] + (val2[:,1:] * learning_rate) / m\n",
        "    \n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
        "    \n",
        "    return J, grad"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8JSqyP_fhr2",
        "outputId": "d9384174-7603-4756-ebae-2a4376abd60b"
      },
      "source": [
        "J, grad = backprop(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.692664026112636, (10285,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGQFhPhEftlQ"
      },
      "source": [
        "We are finally ready to train our network and use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa5YS9gefk4E",
        "outputId": "1bd42f14-ff09-4198-f837-37dddf44ddee"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# minimize the objective function\n",
        "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate), \n",
        "                method='TNC', jac=True, options={'maxiter': 250})\n",
        "fmin"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     fun: 0.3775962670550885\n",
              "     jac: array([ 2.74217663e-05, -6.01173651e-06,  1.66093658e-06, ...,\n",
              "       -1.66378210e-04, -6.04143793e-04, -2.36458217e-04])\n",
              " message: 'Max. number of function evaluations reached'\n",
              "    nfev: 250\n",
              "     nit: 19\n",
              "  status: 3\n",
              " success: False\n",
              "       x: array([-0.08399857, -0.03005868,  0.00830468, ...,  0.69877635,\n",
              "        1.79638938, -0.98605801])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaLB391Lf91C"
      },
      "source": [
        "Since the objective function is unlikely to converge completely, we limit the number of iterations. Our total cost has fallen below 0.5, which is a good indicator that the algorithm is working properly. Let's use the parameters it finds and forward it through the network to get some predictions.\n",
        "\n",
        "\n",
        "Let's use the parameters it finds and propagate it forward through the network to obtain predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OflfYKeofzKD",
        "outputId": "7f1489a8-61af-4aa2-b109-58e7232aa27c"
      },
      "source": [
        "X = np.matrix(X)\n",
        "val1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "val2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "a1, z2, a2, z3, h = forward_propagate(X, val1, val2)\n",
        "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
        "y_pred\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10],\n",
              "       [10],\n",
              "       [10],\n",
              "       ...,\n",
              "       [ 9],\n",
              "       [ 9],\n",
              "       [ 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3-D6JK-gQ21"
      },
      "source": [
        "Finally, we can calculate the accuracy and see how well the neural network we have trained performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrkAdBrWgSI-",
        "outputId": "b0e0d9db-378c-45e9-b8fb-d58b6aa51ddd"
      },
      "source": [
        "correct = [1 if a == b else 0 for (a,b) in zip(y_pred, y)]\n",
        "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "print('accuracy = {0}%'.format(accuracy * 100))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 98.28%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1kHuRjog0b9"
      },
      "source": [
        "We have successfully implemented a basic backpropagation neural network and used it to classify handwritten digital images. "
      ]
    }
  ]
}